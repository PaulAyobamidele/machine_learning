{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfccbc90",
   "metadata": {},
   "source": [
    "PCA algorithm for dimensionality reduction:\n",
    "let's summarize the approach in a few simple steps:\n",
    "1. Standardize the d-dimensional dataset.\n",
    "2. Construct the covariance matrix.\n",
    "3. Decompose the covariance matrix into its eigenvectors and eigenvalues.\n",
    "4. Sort the eigenvalues by decreasing order to rank the corresponding\n",
    "eigenvectors.\n",
    "5. Select k eigenvectors which correspond to the k largest eigenvalues, where k\n",
    "is the dimensionality of the new feature subspace ( k d≤ ).\n",
    "6. Construct a projection matrix W from the \"top\" k eigenvectors.\n",
    "7. Transform the d-dimensional input dataset X using the projection matrix W\n",
    "to obtain the new k-dimensional feature subspace."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cceaaa",
   "metadata": {},
   "source": [
    "In the following sections, we will perform a PCA step by step, using Python as a learning exercise. Then, we will see how to perform a PCA more conveniently using scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32db1eb5",
   "metadata": {},
   "source": [
    "First, we will start by loading the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43510988",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.datasets import load_diabetes\n",
    "#diabetes = load_diabetes()\n",
    "#X = diabetes.data\n",
    "#y = diabetes.target\n",
    "\n",
    "import pandas as pd\n",
    "#df_diabetes = pd.DataFrame(diabetes.data, diabetes.target, diabetes.feature_names)\n",
    "df_wine = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data',header=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c7a607",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7149e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,stratify=y, random_state=0)\n",
    "\n",
    "# standardize the features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "X_test_std = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8e7109",
   "metadata": {},
   "source": [
    "Covariance matrix, eigen vectors, & values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836ed73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "cov_mat = np.cov(X_train_std.T)\n",
    "\n",
    "eigen_vals, eigen_vecs = np.linalg.eig(cov_mat)\n",
    "print('\\nEigenvalues \\n%s' % eigen_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5357f19",
   "metadata": {},
   "source": [
    "Using the `numpy.cov` function, we computed the covariance matrix of the standardized training dataset. \n",
    "\n",
    "Using the `linalg.eig` function, we performed the eigendecomposition, which yielded a vector (`eigen_vals`) consisting of 13 eigenvalues and the corresponding eigenvectors stored as columns in a 13 x 13-dimensional matrix (`eigen_vecs`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e476d644",
   "metadata": {},
   "source": [
    "Since we want to reduce the dimensionality of our dataset by compressing it onto a new feature subspace, we only select the subset of the eigenvectors (principal components) that contains most of the information (variance). \n",
    "\n",
    "The eigenvalues define the magnitude of the eigenvectors, so we have to sort the eigenvalues by decreasing magnitude; we are interested in the top k eigenvectors based on the values of their corresponding eigenvalues.\n",
    "\n",
    "But before we collect those k most informative eigenvectors, let us plot the variance explained ratios of the eigenvalues. \n",
    "\n",
    "The variance explained ratio of an eigenvalue $λ_j$ is simply the fraction of an eigenvalue $λ_j$ and the total sum of the eigenvalues:\n",
    "\n",
    "$$ \\frac{λ_j}{\\sum_{j=1}^d{λ_j}} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43a1044",
   "metadata": {},
   "source": [
    "Using the NumPy `cumsum` function, we can then calculate the cumulative sum of explained variances, which we will then plot via Matplotlib's `step` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acdc440",
   "metadata": {},
   "outputs": [],
   "source": [
    "tot = sum(eigen_vals)\n",
    "var_exp = [(i / tot) for i in sorted(eigen_vals, reverse=True)]\n",
    "cum_var_exp = np.cumsum(var_exp)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.bar(range(1,14), var_exp, alpha=0.5, align='center',label='individual explained variance')\n",
    "plt.step(range(1,14), cum_var_exp, where='mid', label='cumulative explained variance')\n",
    "plt.ylabel('Explained variance ratio')\n",
    "plt.xlabel('Principal component index')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19527a17",
   "metadata": {},
   "source": [
    "The resulting plot indicates that the first principal component alone accounts for approximately 40 percent of the variance. \n",
    "\n",
    "Also, we can see that the first two principal components combined explain almost 60 percent of the variance in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48761bd6",
   "metadata": {},
   "source": [
    "We should remind ourselves that PCA is an unsupervised method, which means that information about the class labels is ignored. \n",
    "\n",
    "The variance measures the spread of values along a feature axis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202fa2d9",
   "metadata": {},
   "source": [
    "## Feature transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58f1481",
   "metadata": {},
   "source": [
    "After we have successfully decomposed the covariance matrix into eigenpairs, let's now proceed with the last three steps to transform our dataset onto the new principal component axes. \n",
    "\n",
    "The remaining steps we are going to tackle in this section are the following ones:\n",
    "\n",
    "\n",
    "Select `k` eigenvectors, which correspond to the `k` largest eigenvalues, where `k` is the dimensionality of the new feature subspace ( $ k ≤ d $).\n",
    "\n",
    "Construct a projection matrix `W` from the \"top\" `k` eigenvectors.\n",
    "\n",
    "Transform the d-dimensional input dataset `X` using the projection matrix `W` to obtain the new k-dimensional feature subspace.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b5714a",
   "metadata": {},
   "source": [
    "Or, in less technical terms, we will sort the eigenpairs by descending order of the eigenvalues, construct a projection matrix from the selected eigenvectors, and use the projection matrix to transform the data onto the lower-dimensional subspace. \n",
    "\n",
    "We start by sorting the eigenpairs by decreasing order of the eigenvalues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27382a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list of (eigenvalue, eigenvector) tuples\n",
    "eigen_pairs = [(np.abs(eigen_vals[i]), eigen_vecs[:, i])\n",
    "    for i in range(len(eigen_vals))]\n",
    "\n",
    "# Sort the (eigenvalue, eigenvector) tuples from high to low\n",
    "eigen_pairs.sort(key=lambda k: k[0], reverse=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bcaa1d",
   "metadata": {},
   "source": [
    "Next, we collect the two eigenvectors that correspond to the two largest eigenvalues, to capture about 60 percent of the variance in this dataset. \n",
    "\n",
    "Note that we only chose two eigenvectors for the purpose of illustration, since we are going to plot the data via a two-dimensional scatter plot later in this subsection. \n",
    "\n",
    "In practice, the number of principal components has to be determined by a trade-off between computational efficiency and the performance of the classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c64264",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.hstack((eigen_pairs[0][1][:, np.newaxis], eigen_pairs[1][1][:, np.newaxis]))\n",
    "print('Matrix W:\\n', w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376dbce6",
   "metadata": {},
   "source": [
    "By executing the preceding code, we have created a 13 x 2-dimensional projection matrix W from the top two eigenvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d087f60c",
   "metadata": {},
   "source": [
    "Using the projection matrix, we can now transform a sample `x` (represented as a 1 x 13-dimensional row vector) onto the PCA subspace (the principal components one and two) obtaining `x'` , now a two-dimensional sample vector consisting of two new features:\n",
    "\n",
    "$$ x' = xW $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf241d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_std[0].dot(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0982da",
   "metadata": {},
   "source": [
    "Similarly, we can transform the entire 124 x 13-dimensional training dataset onto the two principal components by calculating the matrix dot product:\n",
    "\n",
    "$$ X' = XW $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1096535e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pca = X_train_std.dot(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f797cd4d",
   "metadata": {},
   "source": [
    "Lastly, let us visualize the transformed our training set, now stored as an 124 x 2-dimensional matrix, in a two-dimensional scatterplot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad31744f",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['r', 'b', 'g']\n",
    "markers = ['s', 'x', 'o']\n",
    "for l, c, m in zip(np.unique(y_train), colors, markers):\n",
    "    plt.scatter(X_train_pca[y_train==l, 0],\n",
    "    X_train_pca[y_train==l, 1],\n",
    "    c=c, label=l, marker=m)\n",
    "plt.xlabel('PC 1')\n",
    "plt.ylabel('PC 2')\n",
    "plt.legend(loc='lower left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a119e57a",
   "metadata": {},
   "source": [
    "As we can see in the resulting plot, the data is more spread along the x-axis—the first principal component—than the second principal component (y-axis), which is consistent with the explained variance ratio plot that we created in the previous subsection. \n",
    "\n",
    "However, we can intuitively see that a linear classifier will likely be able\n",
    "to separate the classes well"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddad0c92",
   "metadata": {},
   "source": [
    "Although we encoded the class label information for the purpose of illustration in the preceding scatter plot, we have to keep in mind that PCA is an unsupervised technique that doesn't use any class label information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5f7a7f",
   "metadata": {},
   "source": [
    "# Principal component analysis in scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbefb20",
   "metadata": {},
   "source": [
    "Although the verbose approach in the previous subsection helped us to follow the inner workings of PCA, we will now discuss how to use the PCA class implemented in scikit-learn. \n",
    "\n",
    "\n",
    "The PCA class is one of scikit-learn's transformer classes, where we first fit the model using the training data before we transform both the training data and the test dataset using the same model parameters. \n",
    "\n",
    "\n",
    "Now, let's use the PCA class from scikit-learn on the Wine training dataset, classify the transformed samples via logistic regression, and visualize the decision regions via the `plot_decision_region` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50ad245",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "lr = LogisticRegression()\n",
    "\n",
    "X_train_pca = pca.fit_transform(X_train_std)\n",
    "X_test_pca = pca.transform(X_test_std)\n",
    "\n",
    "lr.fit(X_train_pca, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb277526",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "def plot_decision_regions(X, y, classifier, resolution=0.02):\n",
    "    # setup marker generator and color map\n",
    "    markers = ('s', 'x', 'o', '^', 'v')\n",
    "    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
    "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "    # plot the decision surface\n",
    "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n",
    "    np.arange(x2_min, x2_max, resolution))\n",
    "    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
    "    Z = Z.reshape(xx1.shape)\n",
    "    plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)\n",
    "    plt.xlim(xx1.min(), xx1.max())\n",
    "    plt.ylim(xx2.min(), xx2.max())\n",
    "    # plot class samples\n",
    "    for idx, cl in enumerate(np.unique(y)):\n",
    "        plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1], alpha=0.6, c=cmap(idx), edgecolor='black',    marker=markers[idx],    label=cl)\n",
    "\n",
    "    \n",
    "\n",
    "plot_decision_regions(X_train_pca, y_train, classifier=lr)\n",
    "plt.xlabel('PC 1')\n",
    "plt.ylabel('PC 2')\n",
    "plt.legend(loc='lower left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71133ba3",
   "metadata": {},
   "source": [
    "By executing the preceding code, we should now see the decision regions for the training data reduced to two principal component axes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f84f25",
   "metadata": {},
   "source": [
    "let's plot the decision regions of the logistic regression on the transformed test dataset to see if it can separate the classes well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87056af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_regions(X_test_pca, y_test, classifier=lr)\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.legend(loc='lower left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6948d780",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "495864cf",
   "metadata": {},
   "source": [
    "If we are interested in the explained variance ratios of the different principal components, we can simply initialize the PCA class with the `n_components` parameter set to `None`, so all principal components are kept and the explained variance ratio can then be accessed via the `explained_variance_ratio_ attribute`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602e2d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=None)\n",
    "X_train_pca = pca.fit_transform(X_train_std)\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68717875",
   "metadata": {},
   "source": [
    "Note that we set `n_components=None` when we initialized the PCA class so that it will return all principal components in a sorted order instead of performing a dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57a5a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the explained variance ratio\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_), marker='o', linestyle='-')\n",
    "plt.title('Explained Variance Ratio vs. Number of Principal Components')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance Ratio')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Compute explained variance ratio and cumulative explained variance ratio\n",
    "var_exp = pca.explained_variance_ratio_\n",
    "cum_var_exp = np.cumsum(var_exp)\n",
    "\n",
    "# Plot the explained variance ratio and cumulative explained variance ratio\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(range(1, len(var_exp) + 1), var_exp, alpha=0.9, align='center', label='individual explained variance')\n",
    "plt.step(range(1, len(cum_var_exp) + 1), cum_var_exp, where='mid', label='cumulative explained variance')\n",
    "plt.ylabel('Explained variance ratio')\n",
    "plt.xlabel('Principal component index')\n",
    "plt.legend(loc='best')\n",
    "plt.title('Explained Variance Ratio and Cumulative Explained Variance')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f97c59",
   "metadata": {},
   "source": [
    "## Example using `iris` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c8cd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "\n",
    "iris = load_iris()\n",
    "iris_df = pd.DataFrame(iris.data, iris.target, iris.feature_names)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3,stratify=iris.target, random_state=0)\n",
    "\n",
    "# standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "X_test_std = scaler.transform(X_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed7d35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "lr = LogisticRegression()\n",
    "\n",
    "X_train_pca = pca.fit_transform(X_train_std)\n",
    "X_test_pca = pca.transform(X_test_std)\n",
    "\n",
    "lr.fit(X_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730ede3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "def plot_decision_regions(X, y, classifier, resolution=0.02):\n",
    "    # setup marker generator and color map\n",
    "    markers = ('s', 'x', 'o', '^', 'v')\n",
    "    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
    "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "    # plot the decision surface\n",
    "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n",
    "    np.arange(x2_min, x2_max, resolution))\n",
    "    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
    "    Z = Z.reshape(xx1.shape)\n",
    "    plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)\n",
    "    plt.xlim(xx1.min(), xx1.max())\n",
    "    plt.ylim(xx2.min(), xx2.max())\n",
    "    # plot class samples\n",
    "    for idx, cl in enumerate(np.unique(y)):\n",
    "        plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1], alpha=0.6, c=cmap(idx), edgecolor='black',    marker=markers[idx],    label=cl)\n",
    "\n",
    "    \n",
    "\n",
    "plot_decision_regions(X_train_pca, y_train, classifier=lr)\n",
    "plt.xlabel('PC 1')\n",
    "plt.ylabel('PC 2')\n",
    "plt.legend(loc='lower left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d066b4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_regions(X_test_pca, y_test, classifier=lr)\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.legend(loc='lower left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b8071b",
   "metadata": {},
   "source": [
    "Reference: Raschka, S., & Mirjalili, V. (2017). Python machine learning second edition. Birmingham, England: Packt Publishing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
